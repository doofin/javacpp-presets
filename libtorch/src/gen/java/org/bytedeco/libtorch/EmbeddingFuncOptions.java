// Targeted by JavaCPP version 1.5.4-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.libtorch;

import org.bytedeco.javacpp.Loader;
import org.bytedeco.javacpp.Pointer;
import org.bytedeco.javacpp.annotation.Namespace;
import org.bytedeco.javacpp.annotation.Properties;


/** Options for {@code torch::nn::functional::embedding}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::embedding(input, weight, F::EmbeddingFuncOptions().norm_type(2.5).scale_grad_by_freq(true).sparse(true));
 *  }</pre> */
@Namespace("torch::nn::functional") @Properties(inherit = org.bytedeco.libtorch.presets.libtorch.class)
public class EmbeddingFuncOptions extends Pointer {
    static { Loader.load(); }
    /** Default native constructor. */
    public EmbeddingFuncOptions() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public EmbeddingFuncOptions(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public EmbeddingFuncOptions(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public EmbeddingFuncOptions position(long position) {
        return (EmbeddingFuncOptions)super.position(position);
    }
    @Override public EmbeddingFuncOptions getPointer(long i) {
        return new EmbeddingFuncOptions(this).position(position + i);
    }

  /** If given, pads the output with the embedding vector at {@code padding_idx} (initialized to zeros) whenever it encounters the index. */
  /** If given, each embedding vector with norm larger than {@code max_norm} is renormalized to have norm {@code max_norm}. */
  /** The p of the p-norm to compute for the {@code max_norm} option. Default {@code }2{@code }. */
  /** If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default {@code }false{@code }. */
  /** If {@code }true{@code }, gradient w.r.t. {@code weight} matrix will be a sparse tensor. */
}
