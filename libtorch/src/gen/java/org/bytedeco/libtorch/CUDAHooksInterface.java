// Targeted by JavaCPP version 1.5.4-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.libtorch;

import org.bytedeco.javacpp.BytePointer;
import org.bytedeco.javacpp.Loader;
import org.bytedeco.javacpp.Pointer;
import org.bytedeco.javacpp.annotation.*;

// #endif

// The CUDAHooksInterface is an omnibus interface for any CUDA functionality
// which we may want to call into from CPU code (and thus must be dynamically
// dispatched, to allow for separate compilation of CUDA code).  How do I
// decide if a function should live in this class?  There are two tests:
//
//  1. Does the *implementation* of this function require linking against
//     CUDA libraries?
//
//  2. Is this function *called* from non-CUDA ATen code?
//
// (2) should filter out many ostensible use-cases, since many times a CUDA
// function provided by ATen is only really ever used by actual CUDA code.
//
// TODO: Consider putting the stub definitions in another class, so that one
// never forgets to implement each virtual function in the real implementation
// in CUDAHooks.  This probably doesn't buy us much though.
@Namespace("at") @Properties(inherit = org.bytedeco.libtorch.presets.libtorch.class)
public class CUDAHooksInterface extends Pointer {
    static { Loader.load(); }
    /** Default native constructor. */
    public CUDAHooksInterface() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public CUDAHooksInterface(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public CUDAHooksInterface(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public CUDAHooksInterface position(long position) {
        return (CUDAHooksInterface)super.position(position);
    }
    @Override public CUDAHooksInterface getPointer(long i) {
        return new CUDAHooksInterface(this).position(position + i);
    }

  // This should never actually be implemented, but it is used to
  // squelch -Werror=non-virtual-dtor

  // Initialize THCState and, transitively, the CUDA state
  

  public native @Const @ByRef Generator getDefaultCUDAGenerator(@Cast("at::DeviceIndex") short device_index/*=-1*/);
  public native @Const @ByRef Generator getDefaultCUDAGenerator();

  public native @ByVal Device getDeviceFromPtr(Pointer data);

  public native @Cast("bool") boolean isPinnedPtr(Pointer data);

  public native @Cast("bool") boolean hasCUDA();

  public native @Cast("bool") boolean hasMAGMA();

  public native @Cast("bool") boolean hasCuDNN();

  public native @Const @ByRef NVRTC nvrtc();

  public native @Cast("int64_t") long current_device();

  public native @Cast("bool") boolean hasPrimaryContext(@Cast("int64_t") long device_index);

  public native @ByVal @Cast("c10::optional<int64_t>*") Pointer getDevceIndexWithPrimaryContext();

  

  

  public native @Cast("bool") boolean compiledWithCuDNN();

  public native @Cast("bool") boolean compiledWithMIOpen();

  public native @Cast("bool") boolean supportsDilatedConvolutionWithCuDNN();

  public native @Cast("bool") boolean supportsDepthwiseConvolutionWithCuDNN();

  public native long versionCuDNN();

  public native @StdString BytePointer showConfig();

  public native double batchnormMinEpsilonCuDNN();

  public native @Cast("int64_t") long cuFFTGetPlanCacheMaxSize(@Cast("int64_t") long device_index);

  public native void cuFFTSetPlanCacheMaxSize(@Cast("int64_t") long device_index, @Cast("int64_t") long max_size);

  public native @Cast("int64_t") long cuFFTGetPlanCacheSize(@Cast("int64_t") long device_index);

  public native void cuFFTClearPlanCache(@Cast("int64_t") long device_index);

  public native int getNumGPUs();
}
