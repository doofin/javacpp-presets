// Targeted by JavaCPP version 1.5.4-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.libtorch;

import org.bytedeco.javacpp.Loader;
import org.bytedeco.javacpp.Pointer;
import org.bytedeco.javacpp.annotation.Namespace;
import org.bytedeco.javacpp.annotation.Properties;


// ============================================================================

/** Options for the {@code Embedding::from_pretrained} function. */
@Namespace("torch::nn") @Properties(inherit = org.bytedeco.libtorch.presets.libtorch.class)
public class EmbeddingFromPretrainedOptions extends Pointer {
    static { Loader.load(); }
    /** Default native constructor. */
    public EmbeddingFromPretrainedOptions() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public EmbeddingFromPretrainedOptions(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public EmbeddingFromPretrainedOptions(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public EmbeddingFromPretrainedOptions position(long position) {
        return (EmbeddingFromPretrainedOptions)super.position(position);
    }
    @Override public EmbeddingFromPretrainedOptions getPointer(long i) {
        return new EmbeddingFromPretrainedOptions(this).position(position + i);
    }

  /** If {@code }true{@code }, the tensor does not get updated in the learning process.
   *  Equivalent to {@code }embedding.weight.requires_grad_(false){@code }. Default: {@code }true{@code } */
  /** If given, pads the output with the embedding vector at {@code padding_idx} (initialized to zeros) whenever it encounters the index. */
  /** If given, each embedding vector with norm larger than {@code max_norm} is renormalized to have norm {@code max_norm}. */
  /** The p of the p-norm to compute for the {@code max_norm} option. Default {@code }2{@code }. */
  /** If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default {@code }false{@code }. */
  /** If {@code }true{@code }, gradient w.r.t. {@code weight} matrix will be a sparse tensor. */
}
