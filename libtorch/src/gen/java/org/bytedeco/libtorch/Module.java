// Targeted by JavaCPP version 1.5.4-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.libtorch;

import org.bytedeco.javacpp.BytePointer;
import org.bytedeco.javacpp.Loader;
import org.bytedeco.javacpp.Pointer;
import org.bytedeco.javacpp.annotation.*;


/** The base class for all modules in PyTorch.
 * 
 *  \rst
 *  .. note::
 *    The design and implementation of this class is largely based on the Python
 *    API. You may want to consult the python documentation for
 *    :py:class:{@code pytorch:torch.nn.Module} for further clarification on certain
 *    methods or behavior.
 *  \endrst
 * 
 *  A {@code Module} is an abstraction over the implementation of some function or
 *  algorithm, possibly associated with some persistent data. A {@code Module} may
 *  contain further {@code Module}s ("submodules"), each with their own
 *  implementation, persistent data and further submodules. {@code Module}s can thus
 *  be said to form a recursive tree structure. A {@code Module} is registered as a
 *  submodule to another {@code Module} by calling {@code register_module()}, typically from
 *  within a parent module's constructor.
 * 
 *  A distinction is made between three kinds of persistent data that may be
 *  associated with a {@code Module}:
 * 
 *  1. *Parameters*: tensors that record gradients, typically weights updated
 *     during the backward step (e.g. the {@code weight} of a {@code Linear} module),
 *  2. *Buffers*: tensors that do not record gradients, typically updated during
 *     the forward step, such as running statistics (e.g. {@code mean} and {@code variance}
 *     in the {@code BatchNorm} module),
 *  3. Any additional state, not necessarily tensors, required for the
 *     implementation or configuration of a {@code Module}.
 * 
 *  The first two kinds of state are special in that they may be registered
 *  with the {@code Module} system to allow convenient access and batch configuration.
 *  For example, registered parameters in any {@code Module} may be iterated over via
 *  the {@code parameters()} accessor. Further, changing the data type of a {@code Module}'s
 *  registered parameters can be done conveniently via {@code Module::to()}, e.g.
 *  {@code module->to(torch::kCUDA)} to move all parameters to GPU memory. Lastly,
 *  registered parameters and buffers are handled specially during a {@code clone()}
 *  operation, which performs a deepcopy of a cloneable {@code Module} hierarchy.
 * 
 *  Parameters are registered with a {@code Module} via {@code register_parameter}. Buffers
 *  are registered separately via {@code register_buffer}. These methods are part of
 *  the public API of {@code Module} and are typically invoked from within a
 *  concrete {@code Module}s constructor. */
@Namespace("torch::nn") @NoOffset @Properties(inherit = org.bytedeco.libtorch.presets.libtorch.class)
public class Module extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public Module(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public Module(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public Module position(long position) {
        return (Module)super.position(position);
    }
    @Override public Module getPointer(long i) {
        return new Module(this).position(position + i);
    }


  /** Tells the base {@code Module} about the name of the submodule. */
  public Module(@StdString BytePointer name) { super((Pointer)null); allocate(name); }
  private native void allocate(@StdString BytePointer name);
  public Module(@StdString String name) { super((Pointer)null); allocate(name); }
  private native void allocate(@StdString String name);

  /** Constructs the module without immediate knowledge of the submodule's name.
   *  The name of the submodule is inferred via RTTI (if possible) the first
   *  time {@code .name()} is invoked. */
  public Module() { super((Pointer)null); allocate(); }
  private native void allocate();

  /** Returns the name of the {@code Module}.
   * 
   *  A {@code Module} has an associated {@code name}, which is a string representation of
   *  the kind of concrete {@code Module} it represents, such as {@code "Linear"} for the
   *  {@code Linear} module. Under most circumstances, this name is automatically
   *  inferred via runtime type information (RTTI). In the unusual circumstance
   *  that you have this feature disabled, you may want to manually name your
   *  {@code Module}s by passing the string name to the {@code Module} base class'
   *  constructor. */
  
  ///
  ///
  public native @StdString @NoException BytePointer name();

  /** Performs a recursive deep copy of the module and all its registered
   *  parameters, buffers and submodules.
   * 
   *  Optionally, this method sets the current device
   *  to the one supplied before cloning. If no device is given, each
   *  parameter and buffer will be moved to the device of its source.
   * 
   *  \rst
   *  .. attention::
   *    Attempting to call the {@code clone()} method inherited from the base {@code Module}
   *    class (the one documented here) will fail. To inherit an actual
   *    implementation of {@code clone()}, you must subclass {@code Cloneable}. {@code Cloneable}
   *    is templatized on the concrete module type, and can thus properly copy a
   *    {@code Module}. This method is provided on the base class' API solely for an
   *    easier-to-use polymorphic interface.
   *  \endrst */
  

  /** Applies the {@code function} to the {@code Module} and recursively to every submodule.
   *  The function must accept a {@code Module&}.
   * 
   *  \rst
   *  .. code-block:: cpp
   *    MyModule module;
   *    module->apply([](nn::Module& module) {
   *      std::cout << module.name() << std::endl;
   *    });
   *  \endrst */
  
  ///
  public native void apply(@Cast("const torch::nn::Module::ModuleApplyFunction*") @ByRef Pointer function);

  /** Applies the {@code function} to the {@code Module} and recursively to every submodule.
   *  The function must accept a {@code const Module&}.
   * 
   *  \rst
   *  .. code-block:: cpp
   *    MyModule module;
   *    module->apply([](const nn::Module& module) {
   *      std::cout << module.name() << std::endl;
   *    });
   *  \endrst */

  /** Applies the {@code function} to the {@code Module} and recursively to every submodule.
   *  The function must accept a {@code const std::string&} for the key of the module,
   *  and a {@code Module&}. The key of the module itself is the empty string. If
   *  {@code name_prefix} is given, it is prepended to every key as
   *  {@code <name_prefix>.<key>} (and just {@code name_prefix} for the module itself).
   * 
   *  \rst
   *  .. code-block:: cpp
   *    MyModule module;
   *    module->apply([](const std::string& key, nn::Module& module) {
   *      std::cout << key << ": " << module.name() << std::endl;
   *    });
   *  \endrst */
  
  ///
  public native void apply(
        @Cast("const torch::nn::Module::NamedModuleApplyFunction*") @ByRef Pointer function,
        @StdString BytePointer name_prefix/*=std::string()*/);
  public native void apply(
        @Cast("const torch::nn::Module::NamedModuleApplyFunction*") @ByRef Pointer function,
        @StdString String name_prefix/*=std::string()*/);

  /** Applies the {@code function} to the {@code Module} and recursively to every submodule.
   *  The function must accept a {@code const std::string&} for the key of the module,
   *  and a {@code const Module&}. The key of the module itself is the empty string.
   *  If {@code name_prefix} is given, it is prepended to every key as
   *  {@code <name_prefix>.<key>} (and just {@code name_prefix} for the module itself).
   * 
   *  \rst
   *  .. code-block:: cpp
   *    MyModule module;
   *    module->apply([](const std::string& key, const nn::Module& module) {
   *      std::cout << key << ": " << module.name() << std::endl;
   *    });
   *  \endrst */

  /** Applies the {@code function} to the {@code Module} and recursively to every submodule.
   *  The function must accept a {@code const std::shared_ptr<Module>&}.
   * 
   *  \rst
   *  .. code-block:: cpp
   *    MyModule module;
   *    module->apply([](const std::shared_ptr<nn::Module>& module) {
   *      std::cout << module->name() << std::endl;
   *    });
   *  \endrst */

  /** Applies the {@code function} to the {@code Module} and recursively to every submodule.
   *  The function must accept a {@code const std::string&} for the key of the module,
   *  and a {@code const std::shared_ptr<Module>&}. The key of the module itself is
   *  the empty string. If {@code name_prefix} is given, it is prepended to every key
   *  as
   *  {@code <name_prefix>.<key>} (and just {@code name_prefix} for the module itself).
   * 
   *  \rst
   *  .. code-block:: cpp
   *    MyModule module;
   *    module->apply([](const std::string& key,
   *                     const std::shared_ptr<nn::Module>& module) {
   *      std::cout << key << ": " << module->name() << std::endl;
   *    });
   *  \endrst */

  /** Returns the parameters of this {@code Module} and if {@code recurse} is true, also
   *  recursively of every submodule. */
  public native @Cast("torch::Tensor*") @StdVector Pointer parameters(@Cast("bool") boolean recurse/*=true*/);
  public native @Cast("torch::Tensor*") @StdVector Pointer parameters();

  /** Returns an {@code OrderedDict} with the parameters of this {@code Module} along with
   *  their keys, and if {@code recurse} is true also recursively of every submodule. */
  public native @ByVal @Cast("torch::OrderedDict<std::string,torch::Tensor>*") Pointer named_parameters(@Cast("bool") boolean recurse/*=true*/);
  public native @ByVal @Cast("torch::OrderedDict<std::string,torch::Tensor>*") Pointer named_parameters();

  /** Returns the buffers of this {@code Module} and if {@code recurse} is true, also
   *  recursively of every submodule. */
  public native @Cast("torch::Tensor*") @StdVector Pointer buffers(@Cast("bool") boolean recurse/*=true*/);
  public native @Cast("torch::Tensor*") @StdVector Pointer buffers();

  /** Returns an {@code OrderedDict} with the buffers of this {@code Module} along with
   *  their keys, and if {@code recurse} is true also recursively of every submodule. */
  
  ///
  public native @ByVal @Cast("torch::OrderedDict<std::string,torch::Tensor>*") Pointer named_buffers(@Cast("bool") boolean recurse/*=true*/);
  public native @ByVal @Cast("torch::OrderedDict<std::string,torch::Tensor>*") Pointer named_buffers();

  /** Returns the submodules of this {@code Module} (the entire submodule hierarchy)
   *  and if {@code include_self} is true, also inserts a {@code shared_ptr} to this module
   *  in the first position.
   * 
   *  \rst
   *  .. warning::
   *    Only pass {@code include_self} as {@code true} if this {@code Module} is stored in a
   *    {@code shared_ptr}! Otherwise an exception will be thrown. You may still call
   *    this method with {@code include_self} set to false if your {@code Module} is not
   *    stored in a {@code shared_ptr}.
   *  \endrst */
  
  ///
  public native @Cast("std::shared_ptr<torch::nn::Module>*") @StdVector Pointer modules(@Cast("bool") boolean include_self/*=true*/);
  public native @Cast("std::shared_ptr<torch::nn::Module>*") @StdVector Pointer modules();

  /** Returns an {@code OrderedDict} of the submodules of this {@code Module} (the entire
   *  submodule hierarchy) and their keys, and if {@code include_self} is true, also
   *  inserts a {@code shared_ptr} to this module in the first position. If
   *  {@code name_prefix} is given, it is prepended to every key as
   *  {@code <name_prefix>.<key>} (and just {@code name_prefix} for the module itself).
   * 
   *  \rst
   *  .. warning::
   *    Only pass {@code include_self} as {@code true} if this {@code Module} is stored in a
   *    {@code shared_ptr}! Otherwise an exception will be thrown. You may still call
   *    this method with {@code include_self} set to false if your {@code Module} is not
   *    stored in a {@code shared_ptr}.
   *  \endrst */
  public native @ByVal @Cast("torch::OrderedDict<std::string,std::shared_ptr<torch::nn::Module> >*") Pointer named_modules(
        @StdString BytePointer name_prefix/*=std::string()*/,
        @Cast("bool") boolean include_self/*=true*/);
  public native @ByVal @Cast("torch::OrderedDict<std::string,std::shared_ptr<torch::nn::Module> >*") Pointer named_modules();
  public native @ByVal @Cast("torch::OrderedDict<std::string,std::shared_ptr<torch::nn::Module> >*") Pointer named_modules(
        @StdString String name_prefix/*=std::string()*/,
        @Cast("bool") boolean include_self/*=true*/);

  /** Returns the direct submodules of this {@code Module}. */
  public native @Cast("std::shared_ptr<torch::nn::Module>*") @StdVector Pointer children();

  /** Returns an {@code OrderedDict} of the direct submodules of this {@code Module} and
   *  their keys. */
  public native @ByVal @Cast("torch::OrderedDict<std::string,std::shared_ptr<torch::nn::Module> >*") Pointer named_children();

  /** Enables "training" mode. */
  public native void train(@Cast("bool") boolean on/*=true*/);
  public native void train();

  /** Calls train(false) to enable "eval" mode.
   *  Do not override this method, override {@code train()} instead. */
  
  ///
  public native void eval();

  /** True if the module is in training mode.
   * 
   *  Every {@code Module} has a boolean associated with it that determines whether
   *  the {@code Module} is currently in *training* mode (set via {@code .train()}) or in
   *  *evaluation* (inference) mode (set via {@code .eval()}). This property is
   *  exposed via {@code is_training()}, and may be used by the implementation of a
   *  concrete module to modify its runtime behavior. See the {@code BatchNorm} or
   *  {@code Dropout} modules for examples of {@code Module}s that use different code paths
   *  depending on this property. */
  
  ///
  public native @Cast("bool") @NoException boolean is_training();

  /** Recursively casts all parameters to the given {@code dtype} and {@code device}.
   * 
   *  If {@code non_blocking} is true and the source is in pinned memory and
   *  destination is on the GPU or vice versa, the copy is performed
   *  asynchronously with respect to the host. Otherwise, the argument has no
   *  effect. */
  
  ///
  public native void to(
        @ByVal Device device,
        @ByVal @Cast("torch::Dtype*") Pointer dtype,
        @Cast("bool") boolean non_blocking/*=false*/);
  public native void to(
        @ByVal Device device,
        @ByVal @Cast("torch::Dtype*") Pointer dtype);

  /** Recursively casts all parameters to the given dtype.
   * 
   *  If {@code non_blocking} is true and the source is in pinned memory and
   *  destination is on the GPU or vice versa, the copy is performed
   *  asynchronously with respect to the host. Otherwise, the argument has no
   *  effect. */
  
  ///
  public native void to(@ByVal @Cast("torch::Dtype*") Pointer dtype, @Cast("bool") boolean non_blocking/*=false*/);
  public native void to(@ByVal @Cast("torch::Dtype*") Pointer dtype);

  /** Recursively moves all parameters to the given device.
   * 
   *  If {@code non_blocking} is true and the source is in pinned memory and
   *  destination is on the GPU or vice versa, the copy is performed
   *  asynchronously with respect to the host. Otherwise, the argument has no
   *  effect. */
  public native void to(@ByVal Device device, @Cast("bool") boolean non_blocking/*=false*/);
  public native void to(@ByVal Device device);

  /** Recursively zeros out the {@code grad} value of each registered parameter. */
  
  ///
  ///
  ///
  public native void zero_grad();

  /** Attempts to cast this {@code Module} to the given {@code ModuleType}.
   * 
   *  This method is useful when calling {@code apply()}.
   *  \rst
   *  .. code-block:: cpp
   * 
   *    void initialize_weights(nn::Module& module) {
   *      torch::NoGradGuard no_grad;
   *      if (auto* linear = module.as<nn::Linear>()) {
   *        linear->weight.normal_(0.0, 0.02);
   *      }
   *    }
   * 
   *    MyModule module;
   *    module->apply(initialize_weights);
   *  \endrst */

  /** Attempts to cast this {@code Module} to the given {@code ModuleType}.
   * 
   *  This method is useful when calling {@code apply()}.
   *  \rst
   *  .. code-block:: cpp
   *    void initialize_weights(nn::Module& module) {
   *      torch::NoGradGuard no_grad;
   *      if (auto* linear = module.as<nn::Linear>()) {
   *        linear->weight.normal_(0.0, 0.02);
   *      }
   *    }
   * 
   *    MyModule module;
   *    module->apply(initialize_weights);
   *  \endrst */

  /** Attempts to cast this {@code Module} to the given {@code ModuleType}.
   * 
   *  This method is useful when calling {@code apply()}.
   *  \rst
   *  .. code-block:: cpp
   * 
   *    void initialize_weights(nn::Module& module) {
   *      torch::NoGradGuard no_grad;
   *      if (auto* linear = module.as<nn::Linear>()) {
   *        linear->weight.normal_(0.0, 0.02);
   *      }
   *    }
   * 
   *    MyModule module;
   *    module.apply(initialize_weights);
   *  \endrst */

  /** Attempts to cast this {@code Module} to the given {@code ModuleType}.
   * 
   *  This method is useful when calling {@code apply()}.
   *  \rst
   *  .. code-block:: cpp
   * 
   *    void initialize_weights(nn::Module& module) {
   *      torch::NoGradGuard no_grad;
   *      if (auto* linear = module.as<nn::Linear>()) {
   *        linear->weight.normal_(0.0, 0.02);
   *      }
   *    }
   * 
   *    MyModule module;
   *    module.apply(initialize_weights);
   *  \endrst */

  /** Serializes the {@code Module} into the given {@code OutputArchive}.
   * 
   *  If the {@code Module} contains unserializable submodules (e.g. {@code nn::Functional}),
   *  those submodules are skipped when serializing. */
  
  ///
  public native void save(@ByRef OutputArchive archive);

  /** Deserializes the {@code Module} from the given {@code InputArchive}.
   * 
   *  If the {@code Module} contains unserializable submodules (e.g. {@code nn::Functional}),
   *  we don't check the existence of those submodules in the {@code InputArchive} when
   *  deserializing. */
  
  ///
  public native void load(@ByRef InputArchive archive);

  /** Streams a pretty representation of the {@code Module} into the given {@code stream}.
   *  By default, this representation will be the name of the module (taken from
   *  {@code name()}), followed by a recursive pretty print of all of the {@code Module}'s
   *  submodules.
   * 
   *  Override this method to change the pretty print. The input
   *  {@code stream} should be returned from the method, to allow easy chaining. */
  public native void pretty_print(@Cast("std::ostream*") @ByRef Pointer stream);

  /** Returns whether the {@code Module} is serializable. */
  
  ///
  ///
  ///
  ///
  public native @Cast("bool") boolean is_serializable();

  /** Registers a parameter with this {@code Module}.
   * 
   *  A parameter should be any gradient-recording tensor used in the
   *  implementation of your {@code Module}. Registering it makes it available to
   *  methods such as {@code parameters()}, {@code clone()} or {@code to().}
   * 
   *  Note that registering an undefined Tensor (e.g. {@code module.register_parameter("param", Tensor())})
   *  is allowed, and is equivalent to {@code module.register_parameter("param", None)} in Python API.
   * 
   *  \rst
   *  .. code-block:: cpp
   * 
   *    MyModule::MyModule() {
   *      weight_ = register_parameter("weight", torch::randn({A, B}));
   *    }
   *  \endrst */
  
  ///
  ///
  ///
  public native @Cast("torch::Tensor*") @ByRef Pointer register_parameter(
        @StdString BytePointer name,
        @ByVal @Cast("torch::Tensor*") Pointer tensor,
        @Cast("bool") boolean requires_grad/*=true*/);
  public native @Cast("torch::Tensor*") @ByRef Pointer register_parameter(
        @StdString BytePointer name,
        @ByVal @Cast("torch::Tensor*") Pointer tensor);
  public native @Cast("torch::Tensor*") @ByRef Pointer register_parameter(
        @StdString String name,
        @ByVal @Cast("torch::Tensor*") Pointer tensor,
        @Cast("bool") boolean requires_grad/*=true*/);
  public native @Cast("torch::Tensor*") @ByRef Pointer register_parameter(
        @StdString String name,
        @ByVal @Cast("torch::Tensor*") Pointer tensor);

  /** Registers a buffer with this {@code Module}.
   * 
   *  A buffer is intended to be state in your module that does not record
   *  gradients, such as running statistics. Registering it makes it available
   *  to methods such as {@code buffers()}, {@code clone()} or {@code to().
   * 
   *  \rst
   *  .. code-block:: cpp
   * 
   *    MyModule::MyModule() {
   *      mean_ = register_buffer("mean", torch::empty({num_features_}));
   *    }
   *  \endrst */
  
  ///
  ///
  ///
  public native @Cast("torch::Tensor*") @ByRef Pointer register_buffer(@StdString BytePointer name, @ByVal @Cast("torch::Tensor*") Pointer tensor);
  public native @Cast("torch::Tensor*") @ByRef Pointer register_buffer(@StdString String name, @ByVal @Cast("torch::Tensor*") Pointer tensor);

  /** Registers a submodule with this {@code Module}.
   * 
   *  Registering a module makes it available to methods such as {@code modules()},
   *  {@code clone()} or {@code to()}.
   * 
   *  \rst
   *  .. code-block:: cpp
   * 
   *    MyModule::MyModule() {
   *      submodule_ = register_module("linear", torch::nn::Linear(3, 4));
   *    }
   *  \endrst */

  /** Registers a submodule with this {@code Module}.
   * 
   *  This method deals with {@code ModuleHolder}s.
   * 
   *  Registering a module makes it available to methods such as {@code modules()},
   *  {@code clone()} or {@code to()}.
   * 
   *  \rst
   *  .. code-block:: cpp
   * 
   *    MyModule::MyModule() {
   *      submodule_ = register_module("linear", torch::nn::Linear(3, 4));
   *    }
   *  \endrst */

  /** Replaces a registered submodule with this {@code Module}.
   * 
   *  This takes care of the registration, if you used submodule members, you should */
  //  assign the submodule as well, i.e. use as
  /**     module->submodule_ = module->replace_module("linear", torch::nn::Linear(3, 4));
  /** It only works when a module of the name is already registered.
  /**
  /** This is useful for replacing a module after initialization, e.g.
  /** for finetuning. */

  /** Replaces a registered submodule with this {@code Module}.
   *  This method deals with {@code ModuleHolder}s.
   * 
   *  This takes care of the registration, if you used submodule members, you should */
  //  assign the submodule as well, i.e. use as
  /**     module->submodule_ = module->replace_module("linear", linear_holder);
  /** It only works when a module of the name is already registered.
  /**
  /** This is useful for replacing a module after initialization, e.g.
  /** for finetuning. */

  /** Unregisters a submodule from this {@code Module}. If there is no such module
   *  with {@code name} an exception is thrown. */
  public native void unregister_module(@StdString BytePointer name);
  public native void unregister_module(@StdString String name);
}
